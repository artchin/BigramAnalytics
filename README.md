# Bigram Analytics Pipeline

**Клиент:** Маркетинговое агентство (SEO-оптимизация контента)  
**Задача:** Анализ текстового корпуса для поиска популярных ключевых фраз  
**Данные:** 4000+ статей (~12GB)

## Бизнес-контекст

Клиенту нужно было проанализировать контент конкурентов для SEO-стратегии. Ручной анализ 4000 статей нереалистичен — требовалось автоматизированное решение.

**Требования:**
- Найти самые частые словосочетания (биграммы) в корпусе
- Считать по документам, а не по вхождениям (Document Frequency)
- Предоставить код скриптов для запуска на кластере
- Время обработки — не более 10 минут

## Решение

**Job 1:** Очистка текста → извлечение биграмм → подсчёт уникальных документов  
**Job 2:** Сортировка по частоте → выбор Top-N

## Document Frequency

| Метрика | Что показывает |
|---------|----------------|
| Term Frequency | "machine learning" — 100 раз в одной статье |
| Document Frequency | "machine learning" — в 3000 из 4000 статей |

## Результаты

```
of the      3847
in the      3652
to the      2891
on the      2234
and the     2156
...
```

**Оптимизация:** подбор числа reducers сократил время обработки с ~10 до ~5 минут.

## Запуск

```bash
./run.sh
```

## Структура проекта

```
BigramAnalytics/
├── mapper.py      # Job 1: извлечение биграмм
├── reducer.py     # Job 1: подсчёт документов
├── mapper2.py     # Job 2: инверсия для сортировки
├── reducer2.py    # Job 2: выбор Top-N
├── run.sh         # Запуск pipeline
└── README.md
```

## Стек

| Технология | Версия | Назначение |
|------------|--------|------------|
| Hadoop HDFS | 3.x | Хранение данных |
| Hadoop YARN | 3.x | Управление ресурсами |
| Hadoop MapReduce | 3.x | Распределённые вычисления |
| Python Streaming API | 3.8+ | Реализация mapper/reducer |

## Что отдано клиенту

- Код pipeline с документацией
- CSV с результатами анализа (Top-100 биграмм)
- Инструкция по повторному запуску
